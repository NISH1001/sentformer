{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2c3644-59e2-404c-9475-8f854c25ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7e00c7-6927-416c-b74c-770130753a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to put the package in sys path\n",
    "# Alternate: make the package pip installable!\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a6d022-2f6a-433c-b3c4-deaa87f3ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d10c88c-3cc9-4b77-926f-d355c81c99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5dd556-74f6-4db7-842f-9c3c7d82592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentform.modeling import SentenceTransformer\n",
    "from sentform.pooling import MeanPooling\n",
    "from sentform.utils import pairwise_cosine_similarity, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b211f80f-bcca-4611-a349-b5e12956765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d4ba8-9da4-4449-8f3f-8212ae2ece12",
   "metadata": {},
   "source": [
    "# SentenceTransformer Embeddings\n",
    "\n",
    "The `SentenceTransformer` is able to take in any backbone that is supported.\n",
    "In general, these backbones are BERT-based / BERT variants which give embeddings for each token.\n",
    "So, to get the embeddings for the whole sentence, we need a mechanism to aggregate these token embeddings.\n",
    "We can use `sentform.pooling.PoolingLayer` to do so. `MeanPooling` is a standard approach to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4bbca3-9ae5-40fb-808b-e78dddf5f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd08d871-d3f5-404c-8ba1-01bccc94c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/npantha/dev/nish/projects/sentence-transformers/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentformer = SentenceTransformer(\n",
    "    backbone=backbone,\n",
    "    pooling_layer=MeanPooling()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a3ed2c-bd33-4642-8bec-432a0b6ec455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (backbone): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooling): MeanPooling()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e73097-a75a-4fd9-ae33-4d77a8a038c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentformer.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935ee134-a820-46bc-904c-dd25d46d68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love cats.\",\n",
    "    \"I don't like mangoes.\",\n",
    "    \"They are using NLP in the company Fetch.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4a6407-1b99-45d1-a213-2fcb1a7bd926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = sentformer.encode(sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e575405-b0c8-461a-85b1-a95abdcb3918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5344,  0.3247, -0.1033,  ..., -0.0295,  0.2302,  0.2154],\n",
       "        [ 0.2443,  0.2077, -0.2987,  ...,  0.1340,  0.0335, -0.0820],\n",
       "        [ 0.0744, -0.1423,  0.2127,  ..., -0.4782,  0.1212,  0.1719]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daafe767-de13-49ab-b4f9-99756c502564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7191, 0.4666],\n",
       "        [0.7191, 1.0000, 0.4731],\n",
       "        [0.4666, 0.4731, 1.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity-check similarity\n",
    "pairwise_cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33537ed8-a640-4c2a-b259-27baa2c7a3e9",
   "metadata": {},
   "source": [
    "# Multi-Task learner\n",
    "\n",
    "Here, we implement `MultiTaskFormer` which takes in any backbone mentioned in the previous section.\n",
    "Plus, it also takes arbitrary number of `NetworkHead`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a3455be-951c-4b4b-abe1-2e7064cb5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentform.modeling import MultiTaskFormer\n",
    "from sentform.heads import ClassificationHead, NERHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6223278-1413-4d4a-94fb-c87f235919cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs fine-tuning of these heads\n",
    "# Left the tuning part for brevity as per assignment\n",
    "multi_tasker = MultiTaskFormer(\n",
    "    heads=[\n",
    "        ClassificationHead(\n",
    "            backbone.config.hidden_size,\n",
    "            num_classes=3,\n",
    "            labels=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "            multi_label=True\n",
    "        ),\n",
    "        NERHead(\n",
    "            backbone.config.hidden_size,\n",
    "            num_tags=3,\n",
    "            ner_tags=[\"Person\", \"Organization\", \"Location\"],\n",
    "            multi_label=False\n",
    "        )\n",
    "    ],\n",
    "    backbone=backbone,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c857fab-a9a6-42b5-8912-bc3c71f6cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = multi_tasker(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f576a0-c774-4685-870d-b65d89d5492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head_0': {'logits': tensor([[-0.1656, -0.3476,  0.4844],\n",
       "          [-0.0076, -0.5315,  0.1446],\n",
       "          [ 0.0027, -0.2253,  0.3223]]),\n",
       "  'predicted_labels': [['Negative'], ['Negative'], ['Positive', 'Negative']]},\n",
       " 'head_1': {'logits': tensor([[[ 0.3650,  0.1345, -0.2514],\n",
       "           [ 0.5034,  0.0140, -0.2033],\n",
       "           [ 0.2411, -0.0171, -0.1203],\n",
       "           [ 0.4246, -0.0356, -0.1672],\n",
       "           [ 0.2063,  0.3407,  0.4070],\n",
       "           [ 0.2124, -0.2770,  0.1768],\n",
       "           [ 0.3093, -0.0320, -0.1235],\n",
       "           [ 0.4071, -0.0214, -0.0909],\n",
       "           [ 0.4120,  0.0069, -0.1272],\n",
       "           [ 0.3855, -0.0453,  0.0112],\n",
       "           [ 0.3581, -0.0378, -0.0568],\n",
       "           [ 0.2946, -0.0275, -0.1267]],\n",
       "  \n",
       "          [[ 0.3164,  0.2235, -0.2897],\n",
       "           [ 0.3439, -0.0085, -0.2358],\n",
       "           [ 0.4287,  0.1661, -0.1273],\n",
       "           [ 0.1695, -0.1226, -0.1536],\n",
       "           [ 0.0375,  0.3355,  0.1454],\n",
       "           [-0.2184,  0.0718,  0.0032],\n",
       "           [ 0.3735,  0.3854, -0.5621],\n",
       "           [ 0.0276, -0.0222,  0.2107],\n",
       "           [ 0.2947,  0.0826,  0.2655],\n",
       "           [ 0.1045, -0.2442,  0.1952],\n",
       "           [ 0.2054,  0.0523, -0.2438],\n",
       "           [ 0.2008,  0.0178, -0.0370]],\n",
       "  \n",
       "          [[ 0.2812,  0.4550,  0.0732],\n",
       "           [-0.0009,  0.4959,  0.1528],\n",
       "           [-0.0480,  0.2546, -0.0688],\n",
       "           [ 0.5086,  0.4361,  0.2522],\n",
       "           [ 0.3296,  0.2812, -0.2129],\n",
       "           [ 0.2483, -0.0355,  0.1682],\n",
       "           [ 0.1648,  0.1665,  0.3122],\n",
       "           [-0.2230,  0.2750,  0.5543],\n",
       "           [ 0.0723,  0.4554, -0.2042],\n",
       "           [ 0.2163,  0.2319,  0.1598],\n",
       "           [ 0.2813,  0.1818,  0.2747],\n",
       "           [ 0.3130,  0.4454,  0.3162]]]),\n",
       "  'predicted_labels': [['Person',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Location',\n",
       "    'Person'],\n",
       "   ['Person',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Organization',\n",
       "    'Organization',\n",
       "    'Organization',\n",
       "    'Location',\n",
       "    'Person',\n",
       "    'Location'],\n",
       "   ['Organization',\n",
       "    'Organization',\n",
       "    'Organization',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Location',\n",
       "    'Location',\n",
       "    'Organization',\n",
       "    'Organization',\n",
       "    'Person',\n",
       "    'Organization']]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1111e27d-8bb6-42fd-ac7f-388e4a699dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love cats.\n",
      "head_0 | Labels: ['Negative'] | Logits shape: torch.Size([3])\n",
      "head_1 | Labels: ['Person', 'Person', 'Person', 'Person', 'Location', 'Person'] | Logits shape: torch.Size([12, 3])\n",
      "-------\n",
      "Sentence: I don't like mangoes.\n",
      "head_0 | Labels: ['Negative'] | Logits shape: torch.Size([3])\n",
      "head_1 | Labels: ['Person', 'Person', 'Person', 'Person', 'Organization', 'Organization', 'Organization', 'Location', 'Person', 'Location'] | Logits shape: torch.Size([12, 3])\n",
      "-------\n",
      "Sentence: They are using NLP in the company Fetch.\n",
      "head_0 | Labels: ['Positive', 'Negative'] | Logits shape: torch.Size([3])\n",
      "head_1 | Labels: ['Organization', 'Organization', 'Organization', 'Person', 'Person', 'Person', 'Location', 'Location', 'Organization', 'Organization', 'Person', 'Organization'] | Logits shape: torch.Size([12, 3])\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    for head_key, head_output in outputs.items():\n",
    "        predicted_labels = head_output[\"predicted_labels\"][i]\n",
    "        logits_shape = head_output[\"logits\"][i].shape\n",
    "        print(f\"{head_key} | Labels: {predicted_labels} | Logits shape: {logits_shape}\")\n",
    "    print(\"-\" * 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84790d32-41fb-4096-94d0-03a385e200ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f705cc-52e0-4eaf-8146-31f0123fda32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
